#!/bin/bash

# Attach to a screen session if one exists, detach if attached
# elsewhere, create new session otherwise
if [ -f $HOME/.screenrc.$HOST_SHORT ]; then
    alias screen="screen -dRR -c $HOME/.screenrc.$HOST_SHORT"
else
    alias screen="screen -dRR"
fi

## Set grep colors
alias grep="grep --colour=auto"

# Allow for LS_COLORS; output filesizes in more readable format
alias ls="ls --color=auto -v -h -N -T 0 --group-directories-first"
# List files in list format, omit project name
alias lsl="ls -o"
# Show hidden files
alias ls.="ls -od .*"
# Show folders
alias lsd="ls -oUd */"
# Show everything
alias lsa="ls -oa"

## Set SVN colors
alias svn="python $HOME/misc/svn-color/svn-color.py"
alias svnlog="svn log -l 5 -v"

## Queue management (LSF for Summit, SLURM for NERSC machines, MOAB for other OLCF machines)
if [ $HOST_SHORT == "summit" -o $HOST_SHORT == "summitdev" ]; then

    ## Custom format
    export LSB_BJOBS_FORMAT="jobid:7 queue:12 user:12 stat:7 slots:8 nexec_host:13 first_host:18 submit_time:13 start_time:13 run_time:-15"

    ## Show only jobs for $USER (default behavior of bjobs)
    alias bjobsu="bjobs -u $USER"
    ## Show all jobs in queue for all users
    alias bjobsa="bjobs -u all"
    ## Show running jobs for all users
    alias bjobsr="bjobs -u all -r"
    ## Show pending jobs for all users
    alias bjobsp="bjobs -u all -p"

    ## Interactive job
    alias bsubi="bsub -P $PROJID -nnodes 1 -W 00:30 -Is $SHELL"
    alias bsubi_2nodes="bsub -P $PROJID -nnodes 2 -W 00:30 -Is $SHELL"
    alias bsubi_8nodes="bsub -P $PROJID -nnodes 8 -W 00:30 -Is $SHELL"

    ## Count available nodes
    if [ $HOST_SHORT == "summitdev" ]; then
        ## This removes known bad nodes from the list
        alias bnodes_ok="bhosts | grep 'ok' | grep -v -e 'r0c0n07' -e 'r0c0n10' -e 'r0c0n12' -e 'r0c1n10' -e 'r0c1n13' -e 'r0c1n14' -e 'r0c1n15' -e 'r0c1n16' -e 'r0c2n09' -e 'r0c2n16' | wc -l"
    else
        alias bnodes_ok="bhosts | grep 'ok' | wc -l"
    fi

    ## Submit directory of job
    alias bjobsls="bjobs -o 'sub_cwd'"

## SLURM aliases for NERSC machines
elif [ ! -z ${NERSC_HOST+x} ]; then

    ## Show only jobs for $USER
    alias sqsu="sqs -u $USER"
    ## Show only jobs for other users in group
    alias sqsg="sqs -u $PROJ_USERS"

    ## Launch interactive job
    if [ $HOST_SHORT == "edison" ]; then
        alias sbatchi="salloc -q debug -t 00:30:00 -N 1"
    elif [ $HOST_SHORT == "cori" ]; then
        alias sbatchi="salloc -q debug -t 00:30:00 -N 1 -C knl,quad,cache"
    fi

    ## Expand max column width for sqs
    alias sqs="sqs -m 30"

else

    ## Show only jobs for $USER
    alias qstatu="qstat -u $USER"
    alias showqu="showq -u $USER"

    ## Show only jobs for other users in group
    alias qstatg="qstat -u $PROJ_USERS"

    ## Interactive job
    alias qsubi="qsub -I -A $PROJID -q debug -l walltime=01:00:00,nodes=1"
    if [ $HOST_SHORT == "rhea" ]; then
        alias qsubi="qsub -I -A $PROJID -q batch -l walltime=01:00:00,nodes=1"
        alias qsubiX="qsub -I -A $PROJID -q batch -l walltime=01:00:00,nodes=1 -X"
        alias qsubi_gpu="qsub -I -A $PROJID -q batch -l walltime=01:00:00,nodes=1 -lpartition=gpu"
        alias qnodes_ok="qnodes | grep -A2 '^rhea[0-9]\+' | grep 'state = free' | wc -l"
    else
        alias qsubi="qsub -I -A $PROJID -q debug -l walltime=01:00:00,nodes=1"
        alias qnodes_ok="qnodes | grep -A2 '^[0-9]\+' | grep 'state = free' | wc -l"
    fi

fi
